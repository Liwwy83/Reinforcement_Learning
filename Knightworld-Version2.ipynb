{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a5d1b1",
   "metadata": {},
   "source": [
    "# Knightworld Version 2\n",
    "\n",
    "Exploring and understanding dynamic programming concepts in a simple grid world environment.\n",
    "\n",
    "Based on the 4x4 Grid World code from Nimish Sanghi's book titled \"Deep Reinforcement Learning with Python\", where moves are single steps in up, right, down or left direction, and end goal is only top left or bottom right corner.\n",
    "\n",
    "We modify the environment to a larger 8x8 chess board with moves changed to a chess knight's \"L\"-shaped move and set the end goal to a particular square (compared to setting it as any of the 4 corners in Knightworld-Version1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436db5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete\n",
    "from contextlib import closing\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabe6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal state can be set to any square with number 0-63\n",
    "TARGET = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnightworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    An 8x8 Chess Board environment with a single Knight piece.\n",
    "    The goal is to reach a specific square on the board in the fewest number of steps. \n",
    "    \n",
    "    The terminal state is TARGET (defined in the previous cell).\n",
    "    \n",
    "    Actions are the allowed Knight moves in a chess game, i.e. \"L\"-shaped.\n",
    "    8 possible actions numbered 0 to 7, in clockwise fashion, with 0 referring to 2 squares to the left and 1 square up.\n",
    "    \n",
    "    Actions going off the edge leave the Knight piece in current state (but still incur a reward of -1).\n",
    "    \n",
    "    Reward of -1 at each step until the Knight piece reaches a corner square \n",
    "    to incentivise the programme to look for the fewest number of steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.shape = (8, 8)\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 8\n",
    "        self.terminal_state = TARGET\n",
    "        \n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            P[s][0] = self._transition_prob(position, [-1,-2])\n",
    "            P[s][1] = self._transition_prob(position, [-2,-1])\n",
    "            P[s][2] = self._transition_prob(position, [-2,1])\n",
    "            P[s][3] = self._transition_prob(position, [-1,2])\n",
    "            P[s][4] = self._transition_prob(position, [1,2])\n",
    "            P[s][5] = self._transition_prob(position, [2,1])\n",
    "            P[s][6] = self._transition_prob(position, [2,-1])\n",
    "            P[s][7] = self._transition_prob(position, [1,-2])\n",
    "            \n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(self.nS) / self.nS\n",
    "        \n",
    "        # Expose model of environment for dynamic programming\n",
    "        # Do not use for model-free learning algorithm\n",
    "        self.P = P\n",
    "        super(KnightworldEnv, self).__init__(self.nS, self.nA, P, isd)\n",
    "\n",
    "    def _transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Model Transitions. Prob is always 1.0.\n",
    "        :param current: Current position on the grid as (row, col)\n",
    "        :param delta: Change in position for the transition\n",
    "        :return: [(1.0, new_state, reward, done)]\n",
    "        \"\"\"\n",
    "        # If stuck in terminal state\n",
    "        current_state = np.ravel_multi_index(tuple(current), self.shape)\n",
    "        if current_state == self.terminal_state:\n",
    "            return [(1.0, current_state, 0, True)]\n",
    "\n",
    "        temp_position = np.array(current) + np.array(delta)\n",
    "        if temp_position[0] < 0 or temp_position[0] > self.shape[0] - 1\\\n",
    "        or temp_position[1] < 0 or temp_position[1] > self.shape[1] - 1:\n",
    "            new_position = current\n",
    "            \n",
    "        else:\n",
    "            new_position = temp_position\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "\n",
    "        is_done = new_state == self.terminal_state\n",
    "        \n",
    "        return [(1.0, new_state, -1, is_done)]\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            elif s == self.terminal_state:\n",
    "                output = \" T \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += '\\n'\n",
    "\n",
    "            outfile.write(output)\n",
    "\n",
    "        outfile.write('\\n')\n",
    "\n",
    "        # No need to return anything for human\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c23ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 6:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 3:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 4 in direction 7:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 5 in direction 4:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 6 in direction 6:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 7 in direction 7:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 8 in direction 7:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 9 in direction 4:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 10 in direction 0:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the moves in the environment\n",
    "kw_env = KnightworldEnv()\n",
    "obs = kw_env.reset()\n",
    "print(\"Initial Knight position on chess board:\")\n",
    "kw_env.render()\n",
    "\n",
    "for j in range(10):\n",
    "    # End if already on TARGET square\n",
    "    if obs == TARGET:\n",
    "        break\n",
    "    action = np.random.choice(kw_env.nA)\n",
    "    kw_env.step(action)\n",
    "    print(f\"Taking step {j+1} in direction {action}:\")\n",
    "    kw_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec43787",
   "metadata": {},
   "source": [
    "Notice how the Knight (x) did not move in some of the steps when the action would have made it land outside the chess board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy. Random in our case\n",
    "        env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)]\n",
    "            env.nS is number of states\n",
    "            env.nA is number of actions\n",
    "        theta: stop evaluation once value function change is less than theta for all states\n",
    "        discount_factor: Gamma discount factor\n",
    "    Returns: Vector of length env.nS representing value function\n",
    "    \"\"\"\n",
    "    # Start with a (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at possible next actions\n",
    "            for a, pi_a in enumerate(policy[s]):\n",
    "                # For each action, look at possible next states\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate expected value as per backup diagram\n",
    "                    v += pi_a * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            V_new[s] = v\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "        V = np.copy(V_new)\n",
    "        # Stop if change is below threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931c05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-107.12, -103.94,  -75.94, -103.96, -104.88, -103.15,  -67.42,\n",
       "         -98.63],\n",
       "       [ -93.54, -107.17, -105.4 , -104.79,    0.  , -102.94, -104.17,\n",
       "        -106.87],\n",
       "       [-107.89, -100.84,  -91.14, -101.38, -106.59, -100.59,  -86.33,\n",
       "         -93.68],\n",
       "       [-106.21, -105.55, -109.14,  -92.35, -103.08,  -90.81, -108.88,\n",
       "        -106.39],\n",
       "       [-109.52, -105.07, -107.34, -105.3 , -108.42, -104.76, -105.55,\n",
       "        -100.39],\n",
       "       [-110.99, -111.19, -108.69, -109.54, -105.47, -109.  , -108.06,\n",
       "        -111.44],\n",
       "       [-111.44, -111.41, -110.08, -111.33, -110.03, -111.31, -107.41,\n",
       "        -111.19],\n",
       "       [-114.64, -113.01, -112.55, -111.22, -111.01, -111.01, -112.82,\n",
       "        -113.68]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate a random policy where every action has equal probability regardless of state\n",
    "np.round(policy_eval(np.ones((64,8)) / 8, KnightworldEnv()).reshape((8,8)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb2c6b",
   "metadata": {},
   "source": [
    "A random policy does poorly, taking over 100 steps for most squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy, V, env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Improve a policy given an environment and a full description of its dynamics and state-values V.\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing policy\n",
    "        V: current state-value for given policy\n",
    "        env: OpenAI env (same as policy_eval)\n",
    "        discount_factor: Gamma discount factor\n",
    "    Returns:\n",
    "        policy: [S, A] shaped matrix representing improved policy\n",
    "        policy_changed: boolean which takes value 'True' if policy was changed\n",
    "    \"\"\"\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"Return idxs of all max values in array\"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "    \n",
    "    policy_changed = False\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    new_policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    # For each state, perform 'greedy improvement'\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.array(policy[s])\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate expected value as per backup diagram\n",
    "                Q[s,a] += prob * (reward + discount_factor * V[next_state])\n",
    "        # Get maximizing actions and set new policy for state s\n",
    "        best_actions = argmax_a(Q[s])\n",
    "        new_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "    if not np.allclose(new_policy[s], policy[s]):\n",
    "        policy_changed = True\n",
    "    return new_policy, policy_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd638be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def policy_iteration(env, discount_factor=1.0, theta=0.00001):\n",
    "    # Initialize a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = policy_eval(policy, env, discount_factor, theta)\n",
    "        policy, changed = policy_improvement(policy, V, env, discount_factor)\n",
    "        if not changed:\n",
    "            V_optimal = policy_eval(policy, env, discount_factor, theta)\n",
    "            print(\"Optimal Policy\\n\", np.round(policy,3))\n",
    "            return np.array(V_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c0efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      " [[0.    0.    0.    0.    0.5   0.5   0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    0.25  0.25  0.25  0.25 ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.5   0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.    0.333 0.333 0.    0.333 0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.2   0.    0.    0.2   0.2   0.2   0.    0.2  ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.2   0.    0.    0.2   0.2   0.    0.2   0.2  ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.5   0.    0.    0.    0.    0.    0.    0.5  ]\n",
      " [0.    0.    0.333 0.333 0.    0.333 0.    0.   ]\n",
      " [0.    0.    0.5   0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.    0.5   0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.    0.167 0.167 0.   ]\n",
      " [0.    0.    0.5   0.    0.    0.    0.    0.5  ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.    0.    0.    0.    0.5  ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.2   0.2   0.2   0.2   0.2   0.    0.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.5   0.    0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.    0.    0.167 0.167 0.167]\n",
      " [0.25  0.25  0.    0.    0.    0.    0.25  0.25 ]\n",
      " [0.    0.    0.5   0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.2   0.2   0.2   0.2   0.2   0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.25  0.25  0.    0.25  0.    0.    0.25 ]\n",
      " [0.5   0.    0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.    0.    0.    0.    0.25 ]\n",
      " [0.5   0.5   0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.25  0.25  0.25  0.25  0.    0.   ]\n",
      " [0.    0.5   0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.    0.333 0.333 0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.    0.333 0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.25  0.25  0.25  0.25  0.    0.    0.   ]\n",
      " [0.    0.333 0.333 0.333 0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.167 0.    0.    0.167]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.167 0.    0.    0.167]\n",
      " [0.333 0.333 0.333 0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.    0.    0.    0.    0.    0.333]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.333 0.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "Optimal V:\n",
      " [[-3. -2. -1. -2. -3. -2. -1. -2.]\n",
      " [-2. -3. -2. -3.  0. -3. -2. -3.]\n",
      " [-3. -2. -1. -2. -3. -2. -1. -2.]\n",
      " [-2. -3. -4. -1. -2. -1. -4. -3.]\n",
      " [-3. -2. -3. -2. -3. -2. -3. -2.]\n",
      " [-4. -3. -2. -3. -2. -3. -2. -3.]\n",
      " [-3. -4. -3. -4. -3. -4. -3. -4.]\n",
      " [-4. -3. -4. -3. -4. -3. -4. -3.]]\n"
     ]
    }
   ],
   "source": [
    "V = policy_iteration(KnightworldEnv())\n",
    "print(\"\\nOptimal V:\\n\", V.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62d7fd",
   "metadata": {},
   "source": [
    "The Knight can reach the target square within 4 moves regardless of where it starts from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac3b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Carry out Value iteration given an environment and the full description of its dynamics\n",
    "    Returns:\n",
    "        policy: [S,A] shaped matrix representing optimal policy\n",
    "        value: [S] length vector representing optimal value\n",
    "    \"\"\"\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"Return idxs of max element in array\"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "    \n",
    "    optimal_policy = np.zeros([env.nS, env.nA])\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"greedy backup\"\n",
    "        for s in range(env.nS):\n",
    "            q = np.zeros(env.nA)\n",
    "            # Look at possible next actions\n",
    "            for a in range(env.nA):\n",
    "                # For each action, look at possible next states to calculate q[s,a]\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate value for each action as per backup diagram\n",
    "                    if not done:\n",
    "                        q[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                    else:\n",
    "                        q[a] += prob * reward\n",
    "            # Find max value over all possible actions and store updated state value\n",
    "            V_new[s] = q.max()\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "        V = np.copy(V_new)\n",
    "        # Stop if change is below threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # V(s) has optimal values. Use these values and one step backup to calculate optimal policy\n",
    "    for s in range(env.nS):\n",
    "        q = np.zeros(env.nA)\n",
    "        # Look at possible next actions\n",
    "        for a in range(env.nA):\n",
    "            # For each action, look at possible next states and calculate q[s,a]\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate the value for each action as per backup diagram\n",
    "                if not done:\n",
    "                    q[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                else:\n",
    "                    q[a] += prob * reward\n",
    "                    \n",
    "        # Find the optimal actions\n",
    "        best_actions = argmax_a(q)\n",
    "        optimal_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "    return optimal_policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d482755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      " [[0.    0.    0.    0.    0.5   0.5   0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    0.25  0.25  0.25  0.25 ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.5   0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.    0.333 0.333 0.    0.333 0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.2   0.    0.    0.2   0.2   0.2   0.    0.2  ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.2   0.    0.    0.2   0.2   0.    0.2   0.2  ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.5   0.    0.    0.    0.    0.    0.    0.5  ]\n",
      " [0.    0.    0.333 0.333 0.    0.333 0.    0.   ]\n",
      " [0.    0.    0.5   0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.    0.5   0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.    0.167 0.167 0.   ]\n",
      " [0.    0.    0.5   0.    0.    0.    0.    0.5  ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.    0.    0.    0.    0.5  ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.2   0.2   0.2   0.2   0.2   0.    0.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.5   0.    0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.    0.    0.167 0.167 0.167]\n",
      " [0.25  0.25  0.    0.    0.    0.    0.25  0.25 ]\n",
      " [0.    0.    0.5   0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.2   0.2   0.2   0.2   0.2   0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.25  0.25  0.    0.25  0.    0.    0.25 ]\n",
      " [0.5   0.    0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.    0.    0.    0.    0.25 ]\n",
      " [0.5   0.5   0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.25  0.25  0.25  0.25  0.    0.   ]\n",
      " [0.    0.5   0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.    0.333 0.333 0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.    0.333 0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.25  0.25  0.25  0.25  0.    0.    0.   ]\n",
      " [0.    0.333 0.333 0.333 0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.167 0.    0.    0.167]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.167 0.    0.    0.167]\n",
      " [0.333 0.333 0.333 0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.    0.    0.    0.    0.    0.333]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.25  0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.333 0.333 0.333 0.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]]\n",
      "\n",
      "Optimal V:\n",
      " [[-3. -2. -1. -2. -3. -2. -1. -2.]\n",
      " [-2. -3. -2. -3.  0. -3. -2. -3.]\n",
      " [-3. -2. -1. -2. -3. -2. -1. -2.]\n",
      " [-2. -3. -4. -1. -2. -1. -4. -3.]\n",
      " [-3. -2. -3. -2. -3. -2. -3. -2.]\n",
      " [-4. -3. -2. -3. -2. -3. -2. -3.]\n",
      " [-3. -4. -3. -4. -3. -4. -3. -4.]\n",
      " [-4. -3. -4. -3. -4. -3. -4. -3.]]\n"
     ]
    }
   ],
   "source": [
    "policy, V = value_iteration(KnightworldEnv())\n",
    "print(\"Optimal Policy:\\n\", np.round(policy, 3))\n",
    "print(\"\\nOptimal V:\\n\", V.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b9672",
   "metadata": {},
   "source": [
    "Exact same results achieved with value iteration compared to policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f03dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Session 1 ===============\n",
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 0:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 0:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 3:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "============== Session 2 ===============\n",
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 0:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "============== Session 3 ===============\n",
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 1:\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 3:\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 4:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "============== Session 4 ===============\n",
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 0:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 4 in direction 3:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "============== Session 5 ===============\n",
      "Initial Knight position on chess board:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 1 in direction 2:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 2 in direction 1:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 3 in direction 3:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  T  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n",
      "Taking step 4 in direction 3:\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  x  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize optimal moves\n",
    "kw_env = KnightworldEnv()\n",
    "\n",
    "# Try 5 different initial positions\n",
    "for i in range(5):\n",
    "    print(f\" Session {i+1} \".center(40, \"=\"))\n",
    "    obs = kw_env.reset()\n",
    "    print(\"Initial Knight position on chess board:\")\n",
    "    kw_env.render()\n",
    "\n",
    "    # Move up to 4 actions\n",
    "    for j in range(4):\n",
    "        # End if already in a corner square\n",
    "        if obs == TARGET:\n",
    "            break\n",
    "            \n",
    "        action = np.argmax(policy[obs])\n",
    "        obs, reward, done, info = kw_env.step(action)\n",
    "        print(f\"Taking step {j+1} in direction {action}:\")\n",
    "        kw_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9a5e4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "With well-written functions, it is easy to modify the game settings, like changing the target square from any of the 4 corners to just one particular square on the chess board (which could be where the opponent king is in an actual chess game)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
