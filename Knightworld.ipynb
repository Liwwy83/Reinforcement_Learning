{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a5d1b1",
   "metadata": {},
   "source": [
    "# Knightworld\n",
    "\n",
    "Exploring and understanding dynamic programming concepts in a simple grid world environment.\n",
    "\n",
    "Based on the 4x4 Grid World code from Nimish Sanghi's book titled \"Deep Reinforcement Learning with Python\", where moves are single steps in up, right, down or left direction, and end goal is only top left or bottom right corner.\n",
    "\n",
    "We modify the environment to a larger 8x8 chess board with moves changed to a chess knight's \"L\"-shaped move and set the end goal as any of the 4 corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436db5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete\n",
    "from contextlib import closing\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnightworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    An 8x8 Chess Board environment with a single Knight piece.\n",
    "    The goal is to reach a corner square on the board in the fewest number of steps. \n",
    "    \n",
    "    Terminal states are any of the 4 corners squares.\n",
    "    \n",
    "    Actions are the allowed Knight moves in a chess game, i.e. \"L\"-shaped.\n",
    "    8 possible actions numbered 0 to 7, in clockwise fashion, with 0 referring to 2 squares to the left and 1 square up.\n",
    "    \n",
    "    Actions going off the edge leave the Knight piece in current state (but still incur a reward of -1).\n",
    "    \n",
    "    Reward of -1 at each step until the Knight piece reaches a corner square \n",
    "    to incentivise the programme to look for the fewest number of steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.shape = (8, 8)\n",
    "        self.nS = np.prod(self.shape)\n",
    "        self.nA = 8\n",
    "        \n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            P[s][0] = self._transition_prob(position, [-1,-2])\n",
    "            P[s][1] = self._transition_prob(position, [-2,-1])\n",
    "            P[s][2] = self._transition_prob(position, [-2,1])\n",
    "            P[s][3] = self._transition_prob(position, [-1,2])\n",
    "            P[s][4] = self._transition_prob(position, [1,2])\n",
    "            P[s][5] = self._transition_prob(position, [2,1])\n",
    "            P[s][6] = self._transition_prob(position, [2,-1])\n",
    "            P[s][7] = self._transition_prob(position, [1,-2])\n",
    "            \n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(self.nS) / self.nS\n",
    "        \n",
    "        # Expose model of environment for dynamic programming\n",
    "        # Do not use for model-free learning algorithm\n",
    "        self.P = P\n",
    "        super(KnightworldEnv, self).__init__(self.nS, self.nA, P, isd)\n",
    "\n",
    "    def _transition_prob(self, current, delta):\n",
    "        \"\"\"\n",
    "        Model Transitions. Prob is always 1.0.\n",
    "        :param current: Current position on the grid as (row, col)\n",
    "        :param delta: Change in position for the transition\n",
    "        :return: [(1.0, new_state, reward, done)]\n",
    "        \"\"\"\n",
    "        # If stuck in terminal state\n",
    "        current_state = np.ravel_multi_index(tuple(current), self.shape)\n",
    "        if (current_state == 0 or \n",
    "            current_state == self.nS - 1 or \n",
    "            current_state == self.shape[1] - 1 or \n",
    "            current_state == self.nS - self.shape[1]):\n",
    "            return [(1.0, current_state, 0, True)]\n",
    "\n",
    "        temp_position = np.array(current) + np.array(delta)\n",
    "        if temp_position[0] < 0 or temp_position[0] > self.shape[0] - 1\\\n",
    "        or temp_position[1] < 0 or temp_position[1] > self.shape[1] - 1:\n",
    "            new_position = current\n",
    "            \n",
    "        else:\n",
    "            new_position = temp_position\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "\n",
    "        is_done = new_state == 0 or new_state == self.nS - 1 \\\n",
    "            or current_state == self.shape[1] - 1 or current_state == self.nS - self.shape[1]\n",
    "        \n",
    "        return [(1.0, new_state, -1, is_done)]\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            elif s == 0 or s == self.nS - 1 \\\n",
    "                or s == self.shape[1] - 1 or s == self.nS - self.shape[1]:\n",
    "                output = \" T \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += '\\n'\n",
    "\n",
    "            outfile.write(output)\n",
    "\n",
    "        outfile.write('\\n')\n",
    "\n",
    "        # No need to return anything for human\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c23ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 4:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 7:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 7:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 6:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 3:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 0:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the moves in the environment\n",
    "kw_env = KnightworldEnv()\n",
    "print(\"Initial Knight position on chess board:\")\n",
    "kw_env.render()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = np.random.choice(kw_env.nA)\n",
    "    kw_env.step(action)\n",
    "    print(f\"Taking a step in direction {action}:\")\n",
    "    kw_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec43787",
   "metadata": {},
   "source": [
    "Notice how the Knight (x) did not move in some of the steps when the action would have made it land outside the chess board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy. Random in our case\n",
    "        env: OpenAI env. env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)]\n",
    "            env.nS is number of states\n",
    "            env.nA is number of actions\n",
    "        theta: stop evaluation once value function change is less than theta for all states\n",
    "        discount_factor: Gamma discount factor\n",
    "    Returns: Vector of length env.nS representing value function\n",
    "    \"\"\"\n",
    "    # Start with a (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at possible next actions\n",
    "            for a, pi_a in enumerate(policy[s]):\n",
    "                # For each action, look at possible next states\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate expected value as per backup diagram\n",
    "                    v += pi_a * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            V_new[s] = v\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "        V = np.copy(V_new)\n",
    "        # Stop if change is below threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931c05c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.  , -73.9 , -70.76, -70.78, -70.78, -70.76, -73.9 ,   0.  ],\n",
       "       [-73.9 , -72.58, -60.  , -70.76, -70.76, -60.  , -72.58, -73.9 ],\n",
       "       [-70.76, -60.  , -72.19, -70.37, -70.37, -72.19, -60.  , -70.76],\n",
       "       [-70.78, -70.76, -70.37, -69.33, -69.33, -70.37, -70.76, -70.78],\n",
       "       [-70.78, -70.76, -70.37, -69.33, -69.33, -70.37, -70.76, -70.78],\n",
       "       [-70.76, -60.  , -72.19, -70.37, -70.37, -72.19, -60.  , -70.76],\n",
       "       [-73.9 , -72.58, -60.  , -70.76, -70.76, -60.  , -72.58, -73.9 ],\n",
       "       [  0.  , -73.9 , -70.76, -70.78, -70.78, -70.76, -73.9 ,   0.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate a random policy where every action has equal probability regardless of state\n",
    "np.round(policy_eval(np.ones((64,8)) / 8, KnightworldEnv()).reshape((8,8)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb2c6b",
   "metadata": {},
   "source": [
    "A random policy does quite poorly, taking on average 60 steps for squares that can reach a corner square in 1 step and over 70 steps for other non-corner squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bedac93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy, V, env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Improve a policy given an environment and a full description of its dynamics and state-values V.\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing policy\n",
    "        V: current state-value for given policy\n",
    "        env: OpenAI env (same as policy_eval)\n",
    "        discount_factor: Gamma discount factor\n",
    "    Returns:\n",
    "        policy: [S, A] shaped matrix representing improved policy\n",
    "        policy_changed: boolean which takes value 'True' if policy was changed\n",
    "    \"\"\"\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"Return idxs of all max values in array\"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "    \n",
    "    policy_changed = False\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    new_policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    # For each state, perform 'greedy improvement'\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.array(policy[s])\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate expected value as per backup diagram\n",
    "                Q[s,a] += prob * (reward + discount_factor * V[next_state])\n",
    "        # Get maximizing actions and set new policy for state s\n",
    "        best_actions = argmax_a(Q[s])\n",
    "        new_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "    if not np.allclose(new_policy[s], policy[s]):\n",
    "        policy_changed = True\n",
    "    return new_policy, policy_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd638be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def policy_iteration(env, discount_factor=1.0, theta=0.00001):\n",
    "    # Initialize a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = policy_eval(policy, env, discount_factor, theta)\n",
    "        policy, changed = policy_improvement(policy, V, env, discount_factor)\n",
    "        if not changed:\n",
    "            V_optimal = policy_eval(policy, env, discount_factor, theta)\n",
    "            print(\"Optimal Policy\\n\", policy)\n",
    "            return np.array(V_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c0efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      " [[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.5   0.5  ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.5   0.5   0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]]\n",
      "\n",
      "Optimal V:\n",
      " [[ 0. -3. -2. -2. -2. -2. -3.  0.]\n",
      " [-3. -3. -1. -2. -2. -1. -3. -3.]\n",
      " [-2. -1. -3. -2. -2. -3. -1. -2.]\n",
      " [-2. -2. -2. -2. -2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2. -2. -2. -2. -2.]\n",
      " [-2. -1. -3. -2. -2. -3. -1. -2.]\n",
      " [-3. -3. -1. -2. -2. -1. -3. -3.]\n",
      " [ 0. -3. -2. -2. -2. -2. -3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "V = policy_iteration(KnightworldEnv())\n",
    "print(\"\\nOptimal V:\\n\", V.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62d7fd",
   "metadata": {},
   "source": [
    "The Knight can reach a corner square within 3 moves no matter where it starts from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac3b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "def value_iteration(env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Carry out Value iteration given an environment and the full description of its dynamics\n",
    "    Returns:\n",
    "        policy: [S,A] shaped matrix representing optimal policy\n",
    "        value: [S] length vector representing optimal value\n",
    "    \"\"\"\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"Return idxs of max element in array\"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "    \n",
    "    optimal_policy = np.zeros([env.nS, env.nA])\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"greedy backup\"\n",
    "        for s in range(env.nS):\n",
    "            q = np.zeros(env.nA)\n",
    "            # Look at possible next actions\n",
    "            for a in range(env.nA):\n",
    "                # For each action, look at possible next states to calculate q[s,a]\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate value for each action as per backup diagram\n",
    "                    if not done:\n",
    "                        q[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                    else:\n",
    "                        q[a] += prob * reward\n",
    "            # Find max value over all possible actions and store updated state value\n",
    "            V_new[s] = q.max()\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "        V = np.copy(V_new)\n",
    "        # Stop if change is below threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # V(s) has optimal values. Use these values and one step backup to calculate optimal policy\n",
    "    for s in range(env.nS):\n",
    "        q = np.zeros(env.nA)\n",
    "        # Look at possible next actions\n",
    "        for a in range(env.nA):\n",
    "            # For each action, look at possible next states and calculate q[s,a]\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate the value for each action as per backup diagram\n",
    "                if not done:\n",
    "                    q[a] += prob * (reward + discount_factor * V[next_state])\n",
    "                else:\n",
    "                    q[a] += prob * reward\n",
    "                    \n",
    "        # Find the optimal actions\n",
    "        best_actions = argmax_a(q)\n",
    "        optimal_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "    return optimal_policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d482755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      " [[0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    0.    0.    0.5   0.    0.5   0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.5   0.    0.5  ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.    0.    0.5   0.    0.5   0.    0.   ]\n",
      " [0.    0.    0.    0.25  0.25  0.25  0.25  0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.25  0.    0.    0.    0.    0.25  0.25  0.25 ]\n",
      " [0.5   0.    0.    0.    0.    0.    0.5   0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.167 0.167 0.167 0.167 0.167 0.167]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.167 0.167 0.    0.    0.167 0.167 0.167 0.167]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.5   0.5   0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.5   0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.5   0.5  ]\n",
      " [0.    0.    0.    0.    0.5   0.5   0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    1.    0.   ]\n",
      " [0.167 0.167 0.167 0.167 0.167 0.167 0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.167 0.167 0.167 0.167 0.    0.    0.167 0.167]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.5   0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.25  0.25  0.25  0.25  0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.    0.    0.    0.    0.25 ]\n",
      " [0.    0.5   0.    0.    0.    0.    0.    0.5  ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]\n",
      " [0.    0.5   0.    0.5   0.    0.    0.    0.   ]\n",
      " [0.    1.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    1.    0.    0.    0.    0.    0.   ]\n",
      " [0.5   0.    0.5   0.    0.    0.    0.    0.   ]\n",
      " [0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125]]\n",
      "\n",
      "Optimal V:\n",
      " [[ 0. -3. -2. -2. -2. -2. -3.  0.]\n",
      " [-3. -3. -1. -2. -2. -1. -3. -3.]\n",
      " [-2. -1. -3. -2. -2. -3. -1. -2.]\n",
      " [-2. -2. -2. -2. -2. -2. -2. -2.]\n",
      " [-2. -2. -2. -2. -2. -2. -2. -2.]\n",
      " [-2. -1. -3. -2. -2. -3. -1. -2.]\n",
      " [-3. -3. -1. -2. -2. -1. -3. -3.]\n",
      " [ 0. -3. -2. -2. -2. -2. -3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "policy, V = value_iteration(KnightworldEnv())\n",
    "print(\"Optimal Policy:\\n\", np.round(policy, 3))\n",
    "print(\"\\nOptimal V:\\n\", V.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b9672",
   "metadata": {},
   "source": [
    "Exact same results achieved with value iteration compared to policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f03dcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Session 1 ===============\n",
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 2:\n",
      "T  o  o  x  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 4:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 3:\n",
      "T  o  o  o  o  o  o  x\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "============== Session 2 ===============\n",
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 3:\n",
      "T  o  o  o  o  o  o  x\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "============== Session 3 ===============\n",
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  x  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 5:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  x\n",
      "\n",
      "============== Session 4 ===============\n",
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  x  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 4:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  x  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 7:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  T\n",
      "\n",
      "============== Session 5 ===============\n",
      "Initial Knight position on chess board:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  x  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 0:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  x  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 7:\n",
      "T  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  x  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n",
      "Taking a step in direction 1:\n",
      "x  o  o  o  o  o  o  T\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o\n",
      "T  o  o  o  o  o  o  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize optimal moves\n",
    "kw_env = KnightworldEnv()\n",
    "\n",
    "# Try 5 different initial positions\n",
    "for i in range(5):\n",
    "    print(f\" Session {i+1} \".center(40, \"=\"))\n",
    "    obs = kw_env.reset()\n",
    "    print(\"Initial Knight position on chess board:\")\n",
    "    kw_env.render()\n",
    "\n",
    "    # Move up to 3 actions\n",
    "    for _ in range(3):\n",
    "        # End if already in a corner square\n",
    "        if (obs == 0 or \n",
    "            obs == kw_env.nS - 1 or \n",
    "            obs == kw_env.shape[1] - 1 or \n",
    "            obs == kw_env.nS - kw_env.shape[1]):\n",
    "            break\n",
    "            \n",
    "        action = np.argmax(policy[obs])\n",
    "        obs, reward, done, info = kw_env.step(action)\n",
    "        print(f\"Taking a step in direction {action}:\")\n",
    "        kw_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9a5e4",
   "metadata": {},
   "source": [
    "From the above diagrams, we can see that the Knight piece always successfully reaches a corner square within 3 moves.\n",
    "\n",
    "## Conclusion\n",
    "This simple reinforcement learning algorithm worked well to find the optimal policy that satifies our simple game requirement.\n",
    "\n",
    "We can further expand upon this idea by introducing other pieces onto the chess board, so the Knight cannot move to those squares that are already occupied by these other chess pieces.\n",
    "\n",
    "Alternatively, we could assign positive rewards to these other pieces so the Knight gains points from capturing them.\n",
    "\n",
    "We could work on both ideas above at the same time by having some pieces as \"enemies\" (to be captured to gain positive points) and other pieces as \"friends\" (cannot move onto those squares).\n",
    "\n",
    "Perhaps eventually, we can work our way closer and closer to an actual chess game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
